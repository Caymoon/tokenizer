**Copyright (C) 2016 [Wang Renxin](https://cn.linkedin.com/pub/wang-renxin/43/494/20). All rights reserved.**

洋文单词以空格天然分词，相比较而言因为一句中文是由连贯的字组成的，分词就麻烦一些。最困难的情况是对二义性句子的分割问题。比如“搜索引擎”这四个字，可以拆成“搜索”和“引擎”，但“索引”也是一个中文词汇。在脏词过滤的逻辑中最简单的做法是不使用分词，用所有脏词和一句话匹配，如果任意一个脏词是这句话的子串就认定为脏句，这种做法虽然避免了漏网之鱼，但是会因过于严格而让正常的句子变成脏句，分词可以改善这种问题。本项目提供一种简单的中文分词算法，并用近乎伪代码的 Python 编写。期待有人写出更好的版本：比如对效率的优化；在分词过程中引入词性的判定，进而过滤掉在词法上有效但语法上无效的节点等。

目前算法支持二义性划分，如词典里有“喜”，“欢”，“喜欢”三个词，则“喜欢”会拆成[喜欢]和[喜, 欢]两种划分；还支持新词划分，对于未知的词汇，会分割成单字。详见代码本身示例。
